serial version u i d.input.bug.user environment.remote environment.error.error code.error source.retriable.detailed message.is input.is bug.is user environment.is remote environment.determine error source.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.pig exception.retriable.set retriable.get error source.set error source.get error code.set error code.get detailed message.set detailed message.to string.
serial version u i d.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.job creation exception.
distinct udf classname.log.m key field.key field positions.m key type.message collector.combiner optimizer.combiner optimizer.get message collector.visit m r op.patch up map.get pre combiner l r.algebraic.algebraic.fix up foreachs.fix project and inputs.set project input.get distinct user func.add key project.change func.fix up rearrange.reset state.
serial version u i d.impl.p o regexp.p o regexp.set implementation.visit.name.set const expr.get next.clone.
serial version u i d.plan.com op.comp operand type.p o filter.p o filter.p o filter.p o filter.get next.name.supports multiple inputs.supports multiple outputs.visit.set plan.get plan.
tuple factory.exec.
exec.output schema.
exec.output schema.get arg to func mapping.
serial version u i d.log.default delimiter.timestamp.heartbeat.get time stamp.set time stamp.is heart beat.set heart beat.timestamped tuple.timestamped tuple.
create record reader.
serial version u i d.log.m is field schema computed.m field schema.expression operator.expression operator.supports multiple outputs.get schema.get field schema.set field schema.unset field schema.regenerate field schema.set field schema computed.get field schema computed.get type.clone.
serial version u i d.m schema.m is schema computed.m type.m requested parallelism.m alias.m plan.m projection map.m is projection map computed.m pinned options.m custom partitioner.log.get custom partitioner.set custom partitioner.logical operator.logical operator.get operator key.set schema.set parent.force schema.unset schema.regenerate schema.set canonical names.get schema.set type.get type.get alias.get alias string.set alias.get requested parallelism.set requested parallelism.pin option.is pinned option.to string.reconcile schema.visit.get plan.set plan.set schema computed.supports multiple outputs.clone.
schema alias validator.validate.
rule plan visitor.visit.
base plan.roots.leaves.operators.operator sub plan.get base plan.add.connect.connect.disconnect.get sinks.get operators.get predecessors.get sources.get successors.remove.size.is equal.create soft link.remove soft link.get soft link predecessors.get soft link successors.insert between.remove and reconnect.replace.
binary expression.get lhs.get rhs.
uid mapping.l o union.get schema.accept.is equal.get input uids.reset uid.
check.transform.report changes.
multi store record counter.multi store counter group.task counter group.fs counter group.map input records.map output records.reduce input records.reduce output records.hdfs bytes written.hdfs bytes read.multi inputs record counter.multi inputs counter group.log.counter name limit.separator.semicolon.pattern.get multi store count.get multi store counter name.get multi inputs counter name.get short name.start collection.stop collection.get empty pig stats.get pig stats.display statistics.update job mro map.accumulate stats.set error message.set error code.set backend exception.is temp file.add failed job stats.add native job stats.add native job stats.accumulate success statistics.
simple echo streaming command.pc.plan tester.test query foreach 1.test query foreach 2.test query foreach 3.test query cogroup 1.test query group all.test query group 2.test query cogroup 2.test query group 3.test query filter no schema.test query split no schema.test query order by no schema.test query limit no schema.test query distinct no schema.test query streaming no schema.test query streaming no schema 1.test query foreach 4.test foreach 5.test query cross no schema.test query union no schema.test query f r join no schema.test query join no schema.test query filter with schema.test query split with schema.test query order by with schema.test query limit with schema.test query distinct with schema.test query streaming with schema.test query streaming with schema 1.test query implicit join with schema.test query cross with schema.test query union with schema.test query f r join schema.test query join with schema.test query cross with mixed schema.test query union with mixed schema.test query f r join with mixed schema.test query join with mixed schema.test query filter with star no schema.test query order by star no schema.test query group by star no schema.test query f r join on star no schema.test query join on star no schema.test query filter star with schema.test query split with star schema.test query order by star with schema.test query group by star with schema.test query f r join on star with schema.test query join on star with schema.test query foreach generate star no schema.test query foreach generate count star no schema.test query foreach generate star no schema 1.test query foreach generate star no schema 2.test query foreach generate star with schema.test query foreach generate count star with schema.test query foreach generate star with schema 1.test query foreach generate star with schema 2.
input file.input file2.pig server.cluster.test merge join.set up.one time tear down.tear down.test recursive file listing.test merge join simplest.test merge join on multi fields.test merge join with expr.test merge join out with schema.test merge join out with filters.test merge join out with projects.test merge join out pipeline.test merge join with nulls.test merge join with m r boundary later.test merge join 3 way.test merge join failure 1.test merge join failure 2.test empty right file.test parallelism.test indexer.test expression.test expression fail.test merge join sch 1.test merge join sch 2.test merge join with comma separated file paths.test merge join empty index.
loop count.pig.null flags.set up.test group count with multiple fields.test simple count.test group count.test group reorder count.test group unique column count.test group duplicate column count.generate input.
log.pig i stream.pig o stream.pig ex result stream.basedir.test param sub preproc.test cmdline param.test file param.test shell command.test pig param not resolved.test undefined param.test substitution within value.test substitution within shell command.test cmdline param priorto declare.test cmdname as param declare.test multiple cmdline param.test file params from multiple files.test same param in multiple files.test multiple params from single file.test empty comment linein configfile.test invalid linein configfile.test valid linesin configfile.test cmdline file combo.test cmdline file combo duplicate.test cmdline file declare combo.test cmdline file declare combo duplicates.test multiple declare scope.test default param.test cmdline file declare default combo duplicates.test multiple paramsin single line.test substitute within literal.test escaping.test cmdline param with inline cmd.test no vars.test complex vals.test comment with param.
cluster.empty dir.inp file2 nums.inp file2 num1 char1 bag.inp file empty.set up.tear down.one time setup.one time tear down.test union on schema same schema.test union on schema filter.test union on schema succ ops.test union on schema cast on byte array.test union on schema scoped column name.test union on schema scoped column name both inp 1.test union on schema scoped column name both inp 2.test union on schema scoped column name neg.test union on schema diff num type.test union on schema no common cols.test union on schema additional column.test union on schema 3 inputs.test union on schema byte array conversions.test union on schema no schema.test union on schema null alias in field schema.check schema ex.test union on schema incompatible types.test union on schema input udfs.test union on schema udf type evolution.test union on schema udf type evolution 2.test union on schema scope multi.test two unions.
u d f context test eval func 2.exec.
static flag.bufsize.available.token begin.bufpos.bufline.bufcolumn.column.line.prev char is c r.prev char is l f.input stream.buffer.max next char ind.in buf.tab size.set tab size.get tab size.expand buff.fill buff.begin token.update line column.read char.get column.get line.get end column.get end line.get begin column.get begin line.backup.simple char stream.simple char stream.simple char stream.re init.re init.re init.simple char stream.simple char stream.simple char stream.simple char stream.simple char stream.simple char stream.re init.re init.re init.re init.re init.re init.get image.get suffix.done.adjust begin line column.
source table vcolumn name.m projection.m num columns.m proj str.m schema.m keys.projection.is virtual column.get virtual column indices.projection.get keys.get schema.get projection schema.get column schema.to string.get num columns.get num columns.get projection str.to schema.get column index.
log.setmask.clearmask.greater icost.lesser icost.small thresh.depth thresh.qsort stack size.last.orig ptr.block size 1 0 0k.block randomised.bytes out.bs buff.bs live.m crc.in use.n in use.seq to unseq.unseq to seq.selector.selector mtf.block.quadrant.zptr.szptr.ftab.n m t f.mtf freq.work factor.work done.work limit.first attempt.n blocks randomised.current char.run length.written.closed.empty file array.block c r c.combined c r c.allowable block size.bs stream.incs.panic.make maps.hb make code lengths.c b zip 2 output stream.c b zip 2 output stream.write.write run.finalize.close.flush.initialize.init block.end block.end compression.hb assign codes.bs set stream.bs finished with stream.bs w.bs put u char.bs putint.bs put int v s.send m t f values.move to front code and send.simple sort.vswap.med 3.q sort 3.main sort.randomise block.do reversible transformation.full gt u.allocate compress structures.generate m t f values.
log.graphs.curr d a g.pig context.scope counter.scope.aggregate warning.is multi query.parse exec type.construct scope.pig server.pig server.pig server.pig server.pig server.add jars from properties.get pig context.debug on.debug off.set default parallel.set batch on.is batch on.is batch empty.execute batch.execute batch ex.discard batch.add path to skip.register function.register function.register streaming command.locate jar from resources.register jar.register code.register query.get cloned graph.register query.register script.register script.register script.register script.print aliases.dump schema.dump schema nested.set job name.set job priority.open iterator.store.store.store ex.explain.explain.capacity.file size.exists file.delete file.rename file.mkdirs.list paths.total hadoop time spent.get aliases.shutdown.get alias key set.get examples.get store plan.execute.execute compiled logical plan.compile lp.compile lp.merge scalars.compile lp.compile lp.compile pp.validate.get plan from alias.
log.nig.scope.multi query optimizer.visit.visit m r op.is diamond m r oper.merge diamond m r oper.merge one map part.merge only mapper splittee.merge only map reduce splittee.merge all map only splittees.is splittee mergeable.get merge list.merge map reduce splittees.merge map reduce splittees.has same map key type.set index on l r in split.merge one map plan with index.merge one reduce plan with index.add shifted key info index.add shifted key info index.merge one combine plan with index.need combiner.create demux plan.merge all map reduce splittees.merge single map reduce splittee.remove and reconnect.merge m r oper properties.is map only.is single load mapper plan.is single predecessor.get split.get m r oper.get store.get demux.get multi query package.
serial version u i d.m tuple factory.log.err result.plans.secondary plans.leaf ops.secondary leaf ops.index.key type.main key type.secondary key type.m is distinct.is cross.m projected cols map.m secondary projected cols map.m fake tuple.m project star.m secondary project star.is key tuple.is secondary key tuple.m projected cols map size.m secondary projected cols map size.use secondary key.strip key from value.p o local rearrange.p o local rearrange.p o local rearrange.p o local rearrange.visit.name.supports multiple inputs.supports multiple outputs.get index.set index.set multi query index.set index.is distinct.set distinct.attach input.get next.detach plans.get key from result.construct l r output.get key type.set key type.get plans.set use secondary key.set plans.set secondary plans.clone.is cross.set cross.get projected cols map.get secondary projected cols map.is project star.is secondary project star.is key tuple.is secondary key tuple.set plans from combiner.set strip key from value.
m ops.m keys.m from edges.m to edges.m soft from edges.m soft to edges.m roots.m leaves.log.operator plan.get roots.get leaves.get operator key.get operator.get keys.add.connect.create soft link.remove soft link.disconnect.remove.trim below.trim below.trim above.trim above.get predecessors.get successors.get soft link predecessors.get soft link successors.path exists.iterator.mark dirty.remove edges.check in plan.merge.merge shared plan.do merge.add as leaf.is single leaf plan.size.insert between.do insert between.replace node.replace.generate new map.remove and reconnect.reconnect successors.reconnect predecessors.replace and add sucessors.replace and add predecessors.remove and reconnect multi succ.dump.explain.swap.push before.push after.
insert enabled.max script size.log.tss.id.script.command line.pig version.hodoop version.script features.feature map.alias map.listeners.start.script state.get.register listener.emit launch started notification.emit jobs submitted notification.emit job started notification.emitjob finished notification.emit job failed notification.emit output completed notification.emit progress updated notification.emit launch completed notification.add settings to conf.set script.set script.set script features.get hadoop version.get pig version.get id.get command line.set command line.get script.set script.set pig feature.set job parents.get script features.get alias.get pig feature.bit set to long.feature long to string.
log.chunk buf size attr.fs input buf size attr.fs output buf size attr.max key size.api version.compression gz.compression lzo.compression none.comparator memcmp.comparator jclass.get chunk buffer size.get f s input buffer size.get f s output buffer size.make comparator.t file.get supported compression algorithms.main.
serial version u i d.m fields.m aliases.m field schemas.log.two level access required.prime list.schema.schema.schema.schema.copy and link.get field.get field sub name match.get field.size.reconcile.equals.clone.hash code.to string.stringify schema.add.get position.get position sub name.get position.add alias.get aliases.print aliases.get fields.castable.equals.merge.merge schema.merge schema.merge alias.merge schemas by alias.merge schema by alias.check null alias.merge field schema first level same alias.merge name spaced alias.get field sub name match throw schema merge exception.generate nested schema.merge prefix schema.merge prefix schema.set schema default type.is two level access required.set two level access required.get pig schema.find field schema.
log.conf compress.default compress.conf min block size.default min block size.conf min split size.default min split size.split slop.conf non datafile prefix.special file prefix.schema file.meta file.key range for default sorted split.block name index.make meta file path.get compression.get min block size.get non data file prefix.get min split size.drop.build index.dump info.dump info.dump info.head to string.main.